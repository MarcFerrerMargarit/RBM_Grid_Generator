{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we want to analyze is styles from a group of elements. In this case we had 4 styles so we want at least one machine for every one that it will be able to analyze the data an generate new data.\n",
    "\n",
    "So the data that we have is a set of vectors that's the output of the app that generate grids. This grids ara a composition of elements so the outuput will be a vector of id. Each grid had 48 elements and each element can have a maximum of 255 elements in it. But is not necessary to had that number, it can be possible to have less. \n",
    "\n",
    "<h4 align=\"center\">$[ x_1, x_2, x_3, ..., x_{48} ]$</h4> \n",
    "<h4 align=\"center\">$x_1 = [id_1,id_2,_id_3, ...,id_{255}]$</h4>\n",
    "\n",
    "Each element of the vector can have a different size from the others and the list of integers are the subcategories of style of the object that was placed in the grid. Each number is a different subcategory of style of the object placed previously.\n",
    "\n",
    "So the first objective is to descompose the input data, a vector which element is a vector of different size, to a list of vectors that will have de same length, 48, and one element in each position. So the main goal is to find the grid wich elements has more IDs so we can set the size of total vectors, $y$ that we need to descompose the data. In case that one element of the vector has lower IDs that the max we gonna put a $0$ as the null value for our data.\n",
    "\n",
    "<h4 align=\"center\">$input = [[id_1^1,id_2^1,_id_3^1, ...,id_{255}^1], [id_1^2,id_2^2,_id_3^2, ...,id_{255}^2], ..., [id_1^48,id_2^48,_id_3^48, ...,id_{255}^48]]$</h4> \n",
    "<h4 align=\"center\">$output = [[id_1^1, id_1^2,..., id_1^{48}],\n",
    "[id_2^1, id_2^2,..., id_2^{48}],..., [0,0,...,id_y^{48}]$</h4> \n",
    "\n",
    "\n",
    "First of all we gonna implement a function that allows us to obtain the max value of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent =  currentdir + '\\RBM'\n",
    "sys.path.insert(0,parent)\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numexpr  as ne\n",
    "import profile\n",
    "import rbm as Rbm\n",
    "import pandas\n",
    "from random import randint\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we got the max length that will be the number of ouput vectors that we wanna obtain from the input. Now we need to transform the input data to the output data format. For now we gonna tet with a test vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11312, 48)\n"
     ]
    }
   ],
   "source": [
    "vector_data = pd.read_json('./Data.json')\n",
    "A = vector_data.values\n",
    "data_modern = []\n",
    "A = A.ravel()\n",
    "length_vectors = len(A)\n",
    "for i in range(A.shape[0]):\n",
    "    len_v = len(A[i])\n",
    "    tmp = []\n",
    "    for j in range(len_v):\n",
    "        tmp.append(A[i][j])\n",
    "    tmp = np.asarray(tmp).ravel()\n",
    "    data_modern.append(tmp)\n",
    "data_modern = np.asarray(data_modern)\n",
    "input_data = []\n",
    "input_data.append(data_modern)\n",
    "input_data = np.asarray(input_data)\n",
    "max_elements = Utils.getMaxLengthForElements(input_data[0])\n",
    "f_data = Utils.transformInputVector(input_data[0], max_elements, length_vectors)\n",
    "for i in range(100):\n",
    "    for k in range((length_vectors*max_elements)):\n",
    "        f_data.append(f_data[k])\n",
    "f_data = np.asarray(f_data)\n",
    "print(f_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this poing we have the data formatted ready for the input for the machine. In this case we gonna use the algorithm of the library sklearn, BernoulliRBM. Also we gonna use the implementation of One Hot Vector to transform our data to One Hot Vectors.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "Now the first step to get the data in the machine is to convert all the data we formatted to one hot vectors. A one hot vector is a vector which is is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(262, sparse=True).fit(f_data)\n",
    "oneHotData = oneHotEncoder.transform(f_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have 3 hot vectors which one represents the previous input data. This is a sparse matrix so is fastest to work than a simple vector and it's easy to show the results.\n",
    "Then we neeed to define the parameters of the machine and fit it with de one hot vectors we had. The parameters are not specified, so we need to try whichc are the best to obtain good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -336.79, time = 92.12s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -168.11, time = 89.90s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -150.86, time = 89.22s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -145.36, time = 88.80s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -164.81, time = 99.21s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -156.48, time = 100.53s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -149.95, time = 96.91s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -167.42, time = 91.46s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -162.57, time = 90.88s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -152.08, time = 89.80s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.01, n_components=262, n_iter=10,\n",
       "       random_state=0, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RBM_Machine = BernoulliRBM(n_components=262, learning_rate=0.01, n_iter=10, random_state=0, verbose=True)\n",
    "RBM_Machine.fit(oneHotData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we got the output. We gonna make as much as the maximum length of samples. Then we gonna format this data and save it to a json in our folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[220, 4, 216, 253, 248, 220], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [237, 121, 202, 129, 121], [12, 12, 10, 239, 10], [115, 188, 115, 3, 241, 218, 237], [177, 177, 239, 245, 218, 246], [1, 245, 8, 219], [246, 237, 121], [240, 176, 9, 135], [219, 237, 176, 219, 237, 219, 14], [198, 5], [218, 9, 218, 183, 14], [78, 218, 218, 78, 237], [153, 245, 237, 10], [10], [245], [218, 199, 13, 199], [218, 207, 219, 220], [8, 218, 218, 245, 218, 218], [253, 13, 136, 219], [246], [130, 245], [247, 219], [176, 115, 240], [245, 245, 246, 219]]\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "for i in range(max_elements):\n",
    "    x_visible = RBM_Machine.gibbs(oneHotData[random.randint(0,oneHotData.shape[0])])\n",
    "    x_visible = x_visible.ravel()\n",
    "    final_vector = []\n",
    "    for k in range(len(x_visible)):\n",
    "        if(x_visible[k] == True):\n",
    "            final_vector.append(1)\n",
    "        else:\n",
    "            final_vector.append(0)\n",
    "    final_Data = []\n",
    "    for i in range(48):\n",
    "        tmp = final_vector[(262*i):((i+1)*262)]\n",
    "        if(1 not in tmp):\n",
    "            index = 0\n",
    "        else:\n",
    "            index = tmp.index(1)\n",
    "        final_Data.append(index)\n",
    "    final_output.append(final_Data)\n",
    "final_output = np.asarray(final_output)\n",
    "final_data_output = []\n",
    "pos = 0\n",
    "for k in range(48):\n",
    "    t = []\n",
    "    for i in range(len(final_output)):\n",
    "        t.append(final_output[i][k])\n",
    "    final_data_output.append(t)\n",
    "final_data_output = np.asarray(final_data_output)\n",
    "output_grid = []\n",
    "for i in range(48):\n",
    "    v = final_data_output[i].tolist()\n",
    "    v_filter = list(filter((0).__ne__,v))\n",
    "    if(len(v_filter) == 0):\n",
    "        v_filter.append(0)\n",
    "    output_grid.append(v_filter)\n",
    "print(output_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs, json \n",
    "b = output_grid\n",
    "json_file = \"./output_grid.json\" \n",
    "json.dump(b, codecs.open(json_file, 'w', encoding='utf-8'), sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(262, sparse=False).fit(f_data)\n",
    "oneHotData = oneHotEncoder.transform(f_data)\n",
    "test_Data_Vector_Aux = np.array(oneHotData, dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visible_dim = oneHotData.shape[1]\n",
    "hidden_dim = 500\n",
    "epochs = 100\n",
    "K = 1\n",
    "lr = 0.1\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbm_ = Rbm.RBM(visible_dim=visible_dim,\n",
    "               hidden_dim=hidden_dim,\n",
    "               seed=42,\n",
    "               mu=0, \n",
    "               sigma=0.3,\n",
    "               monitor_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLast epoch: 99 \ttime per epoch: 33.55\ttotal time: 3179.17\n",
      "\tTraining finished\n",
      "\n",
      "\n",
      "Wall time: 52min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rbm_.fit(test_Data_Vector_Aux, \n",
    "         method='vectorized_CDK',\n",
    "         K=K,\n",
    "         lr=0.01,\n",
    "         epochs=100,\n",
    "         batch_size=128,\n",
    "         plot_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[219], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [237], [0], [219], [219], [237, 246, 218, 219], [236], [0], [218], [0], [219], [0], [10], [219], [219], [246], [219], [14, 246], [0], [220], [245], [0], [0], [237]]\n"
     ]
    }
   ],
   "source": [
    "final_output = []\n",
    "for i in range(max_elements):\n",
    "    x_visible, x_h = rbm_.sample_visible_from_visible(oneHotData[random.randint(0,oneHotData.shape[0])])\n",
    "    x_visible = x_visible.ravel()\n",
    "    final_vector = []\n",
    "    for k in range(len(x_visible)):\n",
    "        if(x_visible[k] == True):\n",
    "            final_vector.append(1)\n",
    "        else:\n",
    "            final_vector.append(0)\n",
    "    final_Data = []\n",
    "    for i in range(48):\n",
    "        tmp = final_vector[(262*i):((i+1)*262)]\n",
    "        if(1 not in tmp):\n",
    "            index = 0\n",
    "        else:\n",
    "            index = tmp.index(1)\n",
    "        final_Data.append(index)\n",
    "    final_output.append(final_Data)\n",
    "final_output = np.asarray(final_output)\n",
    "final_data_output = []\n",
    "for k in range(48):\n",
    "    t = []\n",
    "    for i in range(len(final_output)):\n",
    "        t.append(final_output[i][k])\n",
    "    final_data_output.append(t)\n",
    "final_data_output = np.asarray(final_data_output)\n",
    "output_grid = []\n",
    "for i in range(48):\n",
    "    v = final_data_output[i].tolist()\n",
    "    v_filter = list(filter((0).__ne__,v))\n",
    "    if(len(v_filter) == 0):\n",
    "        v_filter.append(0)\n",
    "    output_grid.append(v_filter)\n",
    "print(output_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = output_grid\n",
    "json_file = \"./output_grid_2.json\" \n",
    "json.dump(b, codecs.open(json_file, 'w', encoding='utf-8'), sort_keys=True, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
